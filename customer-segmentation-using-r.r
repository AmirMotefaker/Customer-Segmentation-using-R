{"metadata":{"kernelspec":{"name":"ir","display_name":"R","language":"R"},"language_info":{"name":"R","codemirror_mode":"r","pygments_lexer":"r","mimetype":"text/x-r-source","file_extension":".r","version":"4.0.5"}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/amirmotefaker/customer-segmentation-using-r?scriptVersionId=123697555\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"# Customer Segmentation using R\n\n- Customer Segmentation is one of the most important applications of unsupervised learning. Using clustering techniques, companies can identify several segments of customers allowing them to target the potential user base. In this machine learning project, we will make use of K-means clustering which is the essential algorithm for clustering unlabeled datasets.\n\n# What is Customer Segmentation\n- Customer Segmentation is the process of dividing of the customer base into several groups of individuals that share a similarity in different ways that are relevant to marketing such as gender, age, interests, and miscellaneous spending habits.\n\n- Companies that deploy customer segmentation are under the notion that every customer has different requirements and requires a specific marketing effort to address them appropriately. Companies aim to gain a deeper approach to the customer they are targeting. Therefore, their aim has to be specific and should be tailored to address the requirements of each and every individual customer. Furthermore, through the data collected, companies can gain a deeper understanding of customer preferences as well as the requirements for discovering valuable segments that would reap them maximum profit. This way, they can strategize their marketing techniques more efficiently and minimize the possibility of risk to their investment.\n\n- The technique of customer segmentation is dependent on several key differentiators that divide customers into groups to be targeted. Data related to demographics, geography, economic status as well as behavioral patterns play a crucial role in determining the company's direction toward addressing the various segments.\n\n- Customers Segmentation has been a subject of revenue for a ton of industry, scholastics, and showcasing pioneers. The expected worth of a client to an organization can be a center fixing in dynamic. One of the huge difficulties in client-based associations is client cognizance, understanding the distinction among them, and scoring them. However, presently with all capacities we have, utilizing new innovations like AI calculation and information treatment we can make an exceptionally amazing structure that permits us to best comprehend clients' needs and practices and act suitably to fulfill their requirements. In the current paper, we propose another model dependent on the RFM model Recency, Frequency, and Monetary and k-mean calculations to determine those difficulties. This model will permit us to utilize grouping, scoring, and conveyance to have an unmistakable thought regarding what move we should make to improve consumer loyalty.\n\n# Five types of customer segmentation\n\n- Demographic Segmentation: This segmentation divides the customer market based on gender, age, occupation, etc.\n- Geographic Segmentation: This segmentation divides the customer market based on country, state, city, and locality.\n- Technographic Segmentation: This segmentation divides the customer market on the basis of apps, software, and devices.\n- Psychographic Segmentation: This segmentation divides the customer segmentation on the basis of attitudes, values, and personal traits.\n- Behavioral Segmentation: This kind of segmentation divides the customer market on the basis of actions, spending habits, and how they consume.\n\n\n# Benefit of Customer Segmentation for Business\n1. More Customer Retention\n    - Customer retention permits you to gain proficiency with an extraordinary arrangement about your clients so you can take into account their necessities all the more proficiently.  A one-on-one association with your clients will assist you with winning fulfilled clients. You can likewise tailor your correspondence relying upon the client lifecycle.\n    \n\n2. Enhances Competitiveness\n    - The better your customer retention is, the more revenue you are likely to achieve. When a business scores a good customer segmentation along with better utilization of its showcasing financial plans, it acquires a serious edge over opponent organizations. On the off chance that you section up your market, you are not able to your clients and as needs are.\n    \n\n3. Establishes Brand Identity\n    - Distinguishing your image will assist your clients with straightforwardly captivating your items. Whenever you have distinguished the vital helpers for your client, for example, plan or cost or user requirements, you can mark your items properly. By dividing your clients, you can make them very much aware of your image.\n    \n\n4. Better Customer Relationship\n    - Breaking down a huge client base into more reasonable pieces, making it simpler to distinguish your intended interest group and dispatch missions to the most significant individuals, utilizing the most important channel. This creates a good customer relationship as they feel welcomed and heard.\n    \n\n5. Leads to Price Optimization\n    - No business is successful without a good customer base. In order to have one, you need to cater to their primary issues like what their income range is while setting a price for a particular product. This is what price optimization is. The better suited your product price range is according to a customer’s budget, the more your customers will buy it.\n    \n\n6. Best Economies of Scale\n    - Economies of scale is a position where you can reach your specific goal at the minimum cost possible. This can be done through efficient customer segmentation as it helps divide customers into different segments and later, you can focus on limited resources being cost-effective.\n    \n\n7. Improves Channel of Distribution\n    - Customer segmentation can help with distinguishing where clients shop and when can educationally shape item dissemination systems, for example, what sort of items are sold at specific outlets. This will eliminate disarray among your colleagues about whom they need to convey the item and at what time.\n    \n\n8. It Allows You to Fine-Tune Your Message\n    - Dividing customers into segments can actually help you identify what kind of audience you are targeting and hence, can help you deliver your message accordingly. One way to convert visitors into customers is by grabbing their attention and this can be done through customer segmentation.\n    \n\n9. Increases Your Revenue\n    - During the way toward gathering your clients into groups, you may find that you have recognized another market portion, which could thus adjust your advertising center and procedure to fit. This channel of ideas from customer preferences is what will help you convince them to buy your product hence generating a greater portion of revenue for you.\n    \n\n10. Increase Brand Awareness\n    - Customizing marketing correspondence for clients prompts a superior connection between the client and the business. This can significantly improve client devotion. Recognizing your client as beyond what another individual from your email data set can go far for your image value.\n    \n\n\nReference: [Understanding the Benefits of Customer Segmentation](https://bython.com/benefits-of-customer-segmentation/)\n","metadata":{"_uuid":"051d70d956493feee0c6d64651c6a088724dca2a","_execution_state":"idle","execution":{"iopub.status.busy":"2023-02-03T16:06:45.107222Z","iopub.execute_input":"2023-02-03T16:06:45.109426Z","iopub.status.idle":"2023-02-03T16:06:45.224448Z"}}},{"cell_type":"markdown","source":"# How to Implement Customer Segmentation in R\n- In the first step of this data science project, we will perform data exploration. We will import the essential packages required for this role and then read our data. Finally, we will go through the input data to gain necessary insights about it.","metadata":{}},{"cell_type":"markdown","source":"# Data Exploration","metadata":{}},{"cell_type":"code","source":"customer_data=read.csv(\"/kaggle/input/customer-segmentation-tutorial-in-python/Mall_Customers.csv\")","metadata":{"execution":{"iopub.status.busy":"2023-02-08T05:17:12.684885Z","iopub.execute_input":"2023-02-08T05:17:12.687272Z","iopub.status.idle":"2023-02-08T05:17:12.806921Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"str(customer_data)","metadata":{"execution":{"iopub.status.busy":"2023-02-08T05:17:12.810461Z","iopub.execute_input":"2023-02-08T05:17:12.849456Z","iopub.status.idle":"2023-02-08T05:17:12.874348Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"names(customer_data)","metadata":{"execution":{"iopub.status.busy":"2023-02-08T05:17:12.877695Z","iopub.execute_input":"2023-02-08T05:17:12.879847Z","iopub.status.idle":"2023-02-08T05:17:12.904667Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- We will now display the first six rows of our dataset using the head() function and use the summary() function to output the summary of it:","metadata":{}},{"cell_type":"code","source":"head(customer_data)","metadata":{"execution":{"iopub.status.busy":"2023-02-08T05:17:12.987776Z","iopub.execute_input":"2023-02-08T05:17:12.989319Z","iopub.status.idle":"2023-02-08T05:17:13.02222Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"summary(customer_data$Age)","metadata":{"execution":{"iopub.status.busy":"2023-02-08T05:17:13.025022Z","iopub.execute_input":"2023-02-08T05:17:13.02642Z","iopub.status.idle":"2023-02-08T05:17:13.047456Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sd(customer_data$Age)","metadata":{"execution":{"iopub.status.busy":"2023-02-08T05:17:13.049823Z","iopub.execute_input":"2023-02-08T05:17:13.051172Z","iopub.status.idle":"2023-02-08T05:17:13.068149Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"summary(customer_data$Annual.Income..k..)","metadata":{"execution":{"iopub.status.busy":"2023-02-08T05:17:13.070358Z","iopub.execute_input":"2023-02-08T05:17:13.071758Z","iopub.status.idle":"2023-02-08T05:17:13.086819Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sd(customer_data$Annual.Income..k..)","metadata":{"execution":{"iopub.status.busy":"2023-02-08T05:17:13.089038Z","iopub.execute_input":"2023-02-08T05:17:13.090348Z","iopub.status.idle":"2023-02-08T05:17:13.105426Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"summary(customer_data$Age)","metadata":{"execution":{"iopub.status.busy":"2023-02-08T05:17:13.11132Z","iopub.execute_input":"2023-02-08T05:17:13.112689Z","iopub.status.idle":"2023-02-08T05:17:13.127681Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sd(customer_data$Spending.Score..1.100.)","metadata":{"execution":{"iopub.status.busy":"2023-02-08T05:17:13.129906Z","iopub.execute_input":"2023-02-08T05:17:13.13123Z","iopub.status.idle":"2023-02-08T05:17:13.145871Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Customer Gender Visualization\n- Create a barplot and a piechart to show the gender distribution across our customer_data dataset.","metadata":{}},{"cell_type":"code","source":"a=table(customer_data$Gender)\nbarplot(a,main=\"Using BarPlot to display Gender Comparision\",\n       ylab=\"Count\",\n       xlab=\"Gender\",\n       col=rainbow(2),\n       legend=rownames(a))","metadata":{"execution":{"iopub.status.busy":"2023-02-08T05:17:13.148116Z","iopub.execute_input":"2023-02-08T05:17:13.14942Z","iopub.status.idle":"2023-02-08T05:17:13.421219Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- From the above barplot, we observe that the number of females is higher than the number of males.","metadata":{}},{"cell_type":"markdown","source":"### Visualize a pie chart to observe the ratio of male and female distribution:","metadata":{}},{"cell_type":"code","source":"pct=round(a/sum(a)*100)\nlbs=paste(c(\"Female\",\"Male\"),\" \",pct,\"%\",sep=\" \")\nlibrary(plotrix)\npie3D(a,labels=lbs,\n   main=\"Pie Chart Depicting Ratio of Female and Male\")","metadata":{"execution":{"iopub.status.busy":"2023-02-08T05:17:13.423666Z","iopub.execute_input":"2023-02-08T05:17:13.425076Z","iopub.status.idle":"2023-02-08T05:17:13.564635Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- From the above graph, we conclude that the percentage of females is 56%, whereas the percentage of male in the customer dataset is 44%.","metadata":{}},{"cell_type":"markdown","source":"# Visualization of Age Distribution\n- Plot a histogram to view the distribution to plot the frequency of customer ages. We will first proceed by taking a summary of the Age variable.","metadata":{}},{"cell_type":"code","source":"summary(customer_data$Age)","metadata":{"execution":{"iopub.status.busy":"2023-02-08T05:17:13.568132Z","iopub.execute_input":"2023-02-08T05:17:13.570362Z","iopub.status.idle":"2023-02-08T05:17:13.627651Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Show Count of Age Class","metadata":{}},{"cell_type":"code","source":"hist(customer_data$Age,\n    col=\"blue\",\n    main=\"Histogram to Show Count of Age Class\",\n    xlab=\"Age Class\",\n    ylab=\"Frequency\",\n    labels=TRUE)","metadata":{"execution":{"iopub.status.busy":"2023-02-08T05:17:13.630909Z","iopub.execute_input":"2023-02-08T05:17:13.632971Z","iopub.status.idle":"2023-02-08T05:17:13.723065Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Descriptive Analysis of Age","metadata":{}},{"cell_type":"code","source":"boxplot(customer_data$Age,\n       col=\"#ff0066\",\n       main=\"Boxplot for Descriptive Analysis of Age\")","metadata":{"execution":{"iopub.status.busy":"2023-02-08T05:17:13.726735Z","iopub.execute_input":"2023-02-08T05:17:13.729163Z","iopub.status.idle":"2023-02-08T05:17:13.816442Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- From the above two visualizations, we conclude that the maximum customer ages are between 30 and 35. The minimum age of customers is 18, whereas, the maximum age is 70.","metadata":{}},{"cell_type":"markdown","source":"# Analysis of the Annual Income of the Customers\n- Create visualizations to analyze the annual income of the customers. We will plot a histogram and then we will proceed to examine this data using a density plot.","metadata":{}},{"cell_type":"markdown","source":"# Histogram for Annual Income","metadata":{}},{"cell_type":"code","source":"summary(customer_data$Annual.Income..k..)\nhist(customer_data$Annual.Income..k..,\n  col=\"#660033\",\n  main=\"Histogram for Annual Income\",\n  xlab=\"Annual Income Class\",\n  ylab=\"Frequency\",\n  labels=TRUE)","metadata":{"execution":{"iopub.status.busy":"2023-02-08T05:17:13.822888Z","iopub.execute_input":"2023-02-08T05:17:13.824336Z","iopub.status.idle":"2023-02-08T05:17:13.915222Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Density Plot for Annual Income","metadata":{}},{"cell_type":"code","source":"plot(density(customer_data$Annual.Income..k..),\n    col=\"yellow\",\n    main=\"Density Plot for Annual Income\",\n    xlab=\"Annual Income Class\",\n    ylab=\"Density\")\npolygon(density(customer_data$Annual.Income..k..),\n        col=\"#ccff66\")","metadata":{"execution":{"iopub.status.busy":"2023-02-08T05:17:13.917459Z","iopub.execute_input":"2023-02-08T05:17:13.918974Z","iopub.status.idle":"2023-02-08T05:17:14.020173Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- From the above descriptive analysis, we conclude that the minimum annual income of the customers is 15 and the maximum income is 137. \n- People earning an average income of 70 have the highest frequency count in our histogram distribution. \n- The average salary of all the customers is 60.56. \n- In the Kernel Density Plot that we displayed above, we observe that the annual income has a normal distribution.","metadata":{}},{"cell_type":"markdown","source":"# Analyzing Spending Score of the Customers","metadata":{}},{"cell_type":"code","source":"summary(customer_data$Spending.Score..1.100.)","metadata":{"execution":{"iopub.status.busy":"2023-02-08T05:17:14.022507Z","iopub.execute_input":"2023-02-08T05:17:14.023972Z","iopub.status.idle":"2023-02-08T05:17:14.044696Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Descriptive Analysis of Spending Score","metadata":{}},{"cell_type":"code","source":"boxplot(customer_data$Spending.Score..1.100.,\n   horizontal=TRUE,\n   col=\"#990000\",\n   main=\"BoxPlot for Descriptive Analysis of Spending Score\")","metadata":{"execution":{"iopub.status.busy":"2023-02-08T05:17:14.048051Z","iopub.execute_input":"2023-02-08T05:17:14.050077Z","iopub.status.idle":"2023-02-08T05:17:14.134315Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Spending Score Class","metadata":{}},{"cell_type":"code","source":"hist(customer_data$Spending.Score..1.100.,\n    main=\"HistoGram for Spending Score\",\n    xlab=\"Spending Score Class\",\n    ylab=\"Frequency\",\n    col=\"#6600cc\",\n    labels=TRUE)","metadata":{"execution":{"iopub.status.busy":"2023-02-08T05:17:14.136743Z","iopub.execute_input":"2023-02-08T05:17:14.138144Z","iopub.status.idle":"2023-02-08T05:17:14.225341Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- The minimum spending score is 1, the maximum is 99 and the average is 50.20. We can see the Descriptive Analysis of the Spending Score is that Min is 1, Max is 99 and avg. is 50.20. From the histogram, we conclude that customers between classes 40 and 50 have the highest spending score among all the classes.","metadata":{}},{"cell_type":"markdown","source":"# K-Means Algorithm\n- K-Means is one of the most popular \"clustering\" algorithms. K-means stores $k$ centroids that it uses to define clusters. A point is considered to be in a particular cluster if it is closer to that cluster's centroid than any other centroid.\n\n- K-Means finds the best centroids by alternating between (1) assigning data points to clusters based on the current centroids (2) chosing centroids (points which are the center of a cluster) based on the current assignment of data points to clusters.\n\n- K-Means is really just the EM (Expectation Maximization) algorithm applied to a particular naive bayes model.\n\nReference: [Stanford](https://stanford.edu/~cpiech/cs221/handouts/kmeans.html)\n\n- While using the k-means clustering algorithm, the first step is to indicate the number of clusters (k) that we wish to produce in the final output. The algorithm starts by selecting k objects from the dataset randomly that will serve as the initial centers for our clusters. These selected objects are the cluster means, also known as centroids. Then, the remaining objects have an assignment of the closest centroid. This centroid is defined by the Euclidean Distance present between the object and the cluster mean. We refer to this step as “cluster assignment”. When the assignment is complete, the algorithm proceeds to calculate the new mean value of each cluster present in the data. After the recalculation of the centers, the observations are checked if they are closer to a different cluster. Using the updated cluster mean, the objects undergo reassignment. This goes on repeatedly through several iterations until the cluster assignments stop altering. The clusters that are present in the current iteration are the same as the ones obtained in the previous iteration.\n\n- The k-means clustering method is an unsupervised machine learning technique used to identify clusters of data objects in a dataset. There are many different types of clustering methods, but k-means is one of the oldest and most approachable. These traits make implementing k-means clustering in Python reasonably straightforward, even for novice programmers and data analysts.\n\n# K-Means clustering\n\n- We specify the number of clusters that we need to create.\n- The algorithm selects k objects at random from the dataset. This object is the initial cluster or mean.\n- The closest centroid obtains the assignment of a new observation. We base this assignment on the Euclidean Distance between the object and the centroid.\n- k clusters in the data points update the centroid through the calculation of the new mean values present in all the data points of the cluster. The kth cluster’s centroid has a length of p that contains the means of all variables for observations in the kth cluster. We denote the number of variables with p.\n- Iterative minimization of the total within the sum of squares. Then through the iterative minimization of the total sum of the square, the assignment stops wavering when we achieve maximum iteration. The default value is 10 which the R software uses for the maximum iterations.\n\n# What Is Clustering\n- Clustering is a set of techniques used to partition data into groups, or clusters. Clusters are loosely defined as groups of data objects that are more similar to other objects in their cluster than they are to data objects in other clusters. In practice, clustering helps identify two qualities of data:\n\n    - Meaningfulness:\n    \n        - Meaningful clusters expand domain knowledge. For example, in the medical field, researchers applied clustering to gene expression experiments. The clustering results identified groups of patients who respond differently to medical treatments.\n\n    - Usefulness\n    \n        - Useful clusters, on the other hand, serve as an intermediate step in a data pipeline. For example, businesses use clustering for customer segmentation. The clustering results segment customers into groups with similar purchase histories, which businesses can then use to create targeted advertising campaigns.\n\n# Understanding the K-Means Algorithm\n- Conventional k-means require only a few steps. The first step is to randomly select k centroids, where k is equal to the number of clusters you choose. Centroids are data points representing the center of a cluster.\n\n- The main element of the algorithm works by a two-step process called expectation maximization. The expectation step assigns each data point to its nearest centroid. Then, the maximization step computes the mean of all the points for each cluster and sets the new centroid.\n\nReference1: [RealPython](https://realpython.com/k-means-clustering-python/#understanding-the-k-means-algorithm)\n\nReference2: [ScienceDirect](https://www.sciencedirect.com/science/article/abs/pii/S0020025522014633)\n\nReference3: [AFIT](https://afit-r.github.io/kmeans_clustering)","metadata":{}},{"cell_type":"markdown","source":"# Determining Optimal Clusters\n- While working with clusters, you need to specify the number of clusters to use. You would like to utilize the optimal number of clusters. To help you in determining the optimal clusters, there are three popular methods:\n    - Elbow method\n    - Silhouette method\n    - Gap statistic","metadata":{}},{"cell_type":"markdown","source":"# Elbow Method\n- In cluster analysis, the elbow method is a heuristic used in determining the number of clusters in a data set. The method consists of plotting the explained variation as a function of the number of clusters and picking the elbow of the curve as the number of clusters to use.\n- The main goal behind cluster partitioning methods like k-means is to define the clusters such that the intra-cluster variation stays minimum.\n\n### Step by Step Elbow Method:\n1) Select the number of clusters for the dataset ( K )\n\n2) Select K number of centroids\n\n3) By calculating the Euclidean distance or Manhattan distance assign the points to the nearest centroid, thus creating K groups\n\n4) Now find the original centroid in each group\n\n5) Again reassign the whole data point based on this new centroid, then repeat step 4 until the position of the centroid doesn’t change.\n\n\nReference: [Analytics Vidhya](https://www.analyticsvidhya.com/blog/2021/01/in-depth-intuition-of-k-means-clustering-algorithm-in-machine-learning/)","metadata":{}},{"cell_type":"code","source":"library(purrr)\nset.seed(123)\n\n# function to calculate total intra-cluster sum of square \niss <- function(k) {\n  kmeans(customer_data[,3:5],k,iter.max=100,nstart=100,algorithm=\"Lloyd\" )$tot.withinss\n}\nk.values <- 1:10\niss_values <- map_dbl(k.values, iss)\nplot(k.values, iss_values,\n    type=\"b\", pch = 19, frame = FALSE, \n    xlab=\"Number of clusters K\",\n    ylab=\"Total intra-clusters sum of squares\")","metadata":{"execution":{"iopub.status.busy":"2023-02-08T05:17:14.227717Z","iopub.execute_input":"2023-02-08T05:17:14.229183Z","iopub.status.idle":"2023-02-08T05:17:14.440106Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- From the above graph, we conclude that 4 is the appropriate number of clusters since it seems to be appearing at the bend in the elbow plot.","metadata":{}},{"cell_type":"markdown","source":"# Average Silhouette Method\n- The average silhouette approach measures the quality of a clustering. That is, it determines how well each object lies within its cluster. A high average silhouette width indicates good clustering. The average silhouette method computes the average silhouette of observations for different values of k. The optimal number of clusters k is the one that maximizes the average silhouette over a range of possible values for k^2\n\n- We can use the silhouette function in the cluster package to compute the average silhouette width. The following code computes this approach for 1-15 clusters. The results show that 2 clusters maximize the average silhouette values with 4 clusters coming in as the second optimal number of clusters.\n\n    - The silhouette coefficient or silhouette score kmeans is a measure of how similar a data point is within-cluster (cohesion) compared to other clusters (separation:\n\n        - Select a range of values of k (say 1 to 10).\n        - Plot Silhouette coefﬁcient for each value of K.\n\nReference: [Analytics Vidhya](https://www.analyticsvidhya.com/blog/2021/05/k-mean-getting-the-optimal-number-of-clusters/)\n\n- With the help of the average silhouette method, we can measure the quality of our clustering operation. With this, we can determine how well within the cluster is the data object. If we obtain a high average silhouette width, it means that we have good clustering. The average silhouette method calculates the mean of silhouette observations for different k values. With the optimal number of k clusters, one can maximize the average silhouette over significant values for k clusters.\n\n- Using the silhouette function in the cluster package, we can compute the average silhouette width using the K-Mean function. Here, the optimal cluster will possess the highest average.","metadata":{}},{"cell_type":"code","source":"library(cluster)  # Finding Groups in Data\nlibrary(gridExtra)  # Miscellaneous Functions for \"Grid\" Graphics\nlibrary(grid)","metadata":{"execution":{"iopub.status.busy":"2023-02-08T05:17:14.442428Z","iopub.execute_input":"2023-02-08T05:17:14.443852Z","iopub.status.idle":"2023-02-08T05:17:14.549149Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"k2<-kmeans(customer_data[,3:5],2,iter.max=100,nstart=50,algorithm=\"Lloyd\")\ns2<-plot(silhouette(k2$cluster,dist(customer_data[,3:5],\"euclidean\")))","metadata":{"execution":{"iopub.status.busy":"2023-02-08T05:17:14.551514Z","iopub.execute_input":"2023-02-08T05:17:14.552863Z","iopub.status.idle":"2023-02-08T05:17:14.660271Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"k3<-kmeans(customer_data[,3:5],3,iter.max=100,nstart=50,algorithm=\"Lloyd\")\ns3<-plot(silhouette(k3$cluster,dist(customer_data[,3:5],\"euclidean\")))","metadata":{"execution":{"iopub.status.busy":"2023-02-08T05:17:14.76882Z","iopub.execute_input":"2023-02-08T05:17:14.770198Z","iopub.status.idle":"2023-02-08T05:17:14.861903Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"k4<-kmeans(customer_data[,3:5],4,iter.max=100,nstart=50,algorithm=\"Lloyd\")\ns4<-plot(silhouette(k4$cluster,dist(customer_data[,3:5],\"euclidean\")))","metadata":{"execution":{"iopub.status.busy":"2023-02-08T05:17:14.864193Z","iopub.execute_input":"2023-02-08T05:17:14.86566Z","iopub.status.idle":"2023-02-08T05:17:14.958479Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"k5<-kmeans(customer_data[,3:5],5,iter.max=100,nstart=50,algorithm=\"Lloyd\")\ns5<-plot(silhouette(k5$cluster,dist(customer_data[,3:5],\"euclidean\")))","metadata":{"execution":{"iopub.status.busy":"2023-02-08T05:17:14.960895Z","iopub.execute_input":"2023-02-08T05:17:14.962377Z","iopub.status.idle":"2023-02-08T05:17:15.059042Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"k6<-kmeans(customer_data[,3:5],6,iter.max=100,nstart=50,algorithm=\"Lloyd\")\ns6<-plot(silhouette(k6$cluster,dist(customer_data[,3:5],\"euclidean\")))","metadata":{"execution":{"iopub.status.busy":"2023-02-08T05:17:15.061655Z","iopub.execute_input":"2023-02-08T05:17:15.06332Z","iopub.status.idle":"2023-02-08T05:17:15.196918Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"k7<-kmeans(customer_data[,3:5],7,iter.max=100,nstart=50,algorithm=\"Lloyd\")\ns7<-plot(silhouette(k7$cluster,dist(customer_data[,3:5],\"euclidean\")))","metadata":{"execution":{"iopub.status.busy":"2023-02-08T05:17:15.200799Z","iopub.execute_input":"2023-02-08T05:17:15.203097Z","iopub.status.idle":"2023-02-08T05:17:15.333938Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"k8<-kmeans(customer_data[,3:5],8,iter.max=100,nstart=50,algorithm=\"Lloyd\")\ns8<-plot(silhouette(k8$cluster,dist(customer_data[,3:5],\"euclidean\")))","metadata":{"execution":{"iopub.status.busy":"2023-02-08T05:17:15.336328Z","iopub.execute_input":"2023-02-08T05:17:15.337867Z","iopub.status.idle":"2023-02-08T05:17:15.446358Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"k9<-kmeans(customer_data[,3:5],9,iter.max=100,nstart=50,algorithm=\"Lloyd\")\ns9<-plot(silhouette(k9$cluster,dist(customer_data[,3:5],\"euclidean\")))","metadata":{"execution":{"iopub.status.busy":"2023-02-08T05:17:15.448719Z","iopub.execute_input":"2023-02-08T05:17:15.450194Z","iopub.status.idle":"2023-02-08T05:17:15.55906Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"k10<-kmeans(customer_data[,3:5],10,iter.max=100,nstart=50,algorithm=\"Lloyd\")\ns10<-plot(silhouette(k10$cluster,dist(customer_data[,3:5],\"euclidean\")))","metadata":{"execution":{"iopub.status.busy":"2023-02-08T05:17:15.561383Z","iopub.execute_input":"2023-02-08T05:17:15.562846Z","iopub.status.idle":"2023-02-08T05:17:15.674385Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Determine and visualize the optimal number of clusters\nReference: [factoextra](https://rpkgs.datanovia.com/factoextra/reference/fviz_nbclust.html)","metadata":{}},{"cell_type":"markdown","source":"### We make use of the fviz_nbclust() function to determine and visualize the optimal number of clusters as follows:","metadata":{}},{"cell_type":"code","source":"packageurl <- \"https://cran.r-project.org/src/contrib/Archive/emmeans/emmeans_1.7.0.tar.gz\"\ninstall.packages(packageurl, repos=NULL, type=\"source\")\ninstall.packages(\"factoextra\")","metadata":{"execution":{"iopub.status.busy":"2023-02-08T05:17:15.676777Z","iopub.execute_input":"2023-02-08T05:17:15.67818Z","iopub.status.idle":"2023-02-08T05:18:32.265694Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"library(NbClust)\nlibrary(factoextra)  # Extract and Visualize the Results of Multivariate Data Analyses","metadata":{"execution":{"iopub.status.busy":"2023-02-08T05:18:32.269605Z","iopub.execute_input":"2023-02-08T05:18:32.271458Z","iopub.status.idle":"2023-02-08T05:18:32.451896Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fviz_nbclust(customer_data[,3:5], kmeans, method = \"silhouette\")","metadata":{"execution":{"iopub.status.busy":"2023-02-08T05:18:32.454201Z","iopub.execute_input":"2023-02-08T05:18:32.455572Z","iopub.status.idle":"2023-02-08T05:18:33.080157Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Gap Statistic Method\n- Abstract The Gap statistic is a standard method for determining the number of clusters in a set of data. The Gap statistic standardizes the graph of log(Wk), where Wk is the within-cluster dispersion, by comparing it to its expectation under an appropriate null reference distribution of the data.\n- The gap statistic has been published by [R. Tibshirani, G. Walther, and T. Hastie (Standford University, 2001)](http://web.stanford.edu/~hastie/Papers/gap.pdf).\n- The ‘gap statistic’ for estimating the number of clusters (groups) in a set of data. The technique uses the output of any clustering algorithm (e.g. K-means or hierarchical), comparing the change in within-cluster dispersion with that expected under an appropriate reference null distribution.\n- Clustering is an important technique in Pattern Analysis to identify distinct groups in data. Due to data being mostly more than three-dimensional, we perform dimensionality reduction methods like PCA or Laplacian Eigenmaps before applying a clustering technique. The data is then available in 2D or 3D and this allows us to visualize the found clusters very nicely to humans.\n\n### K-Means\n\n- K-Means performs three steps. But first you need to pre-define the number of K. Those cluster points are often called Centroids.\n\n    1) (Re-)assign each data point to its nearest centroid, by calculating the euclidian distance between all points to all centroids.\n\n    2) Calculate the mean for each centroid based on all respective data points and move the centroid in the middle of all his assigned data points.\n\n    3) Go to 1) until the convergence criterion is fulfilled. In my case, I calculate the within-cluster distance between all points to the re-assigned centroid mean. After a new iteration, if all centroids together moved less than 0.01, so basically nothing happens anymore, the convergence criterion is performed.\n\n\n### Gap Statistic\n- Even if Gap-Statistics is a good approach to finding a suitable K, it is still not perfect. For example, we needed to introduce a new hyperparameter, namely the number of K for which the W_uniform is simulated on. We can’t be sure what the ideal value for this is. Furthermore, the random initialization of the centroids can lead to an over- or underestimation of K*.\n\n- But by knowing all of the aspects of Gap-Statistics, the best is to apply it and then run the Gap-Statistic plot a couple of times. Taking the average of the Gap-Statistics can be an increased evaluation criterion.\n\nReference: [K-means Cluster Analysis](https://uc-r.github.io/kmeans_clustering#gap)\n\n- We can use Gap Statistic Method for any of the clustering methods like K-means, hierarchical clustering, etc. Using the gap statistic, one can compare the total intracluster variation for different values of k along with their expected values under the null reference distribution of data. With the help of Monte Carlo simulations, one can produce the sample dataset. For each variable in the dataset, we can calculate the range between min(xi) and max (xj) through which we can produce values uniformly from interval lower bound to upper bound.\n\n- For computing the gap statistics method we can utilize the clusGap function for providing gap statistics as well as standard error for a given output.","metadata":{}},{"cell_type":"code","source":"set.seed(125)\nstat_gap <- clusGap(customer_data[,3:5], FUN = kmeans, nstart = 25,\n            K.max = 10, B = 50)\nfviz_gap_stat(stat_gap)","metadata":{"execution":{"iopub.status.busy":"2023-02-08T05:18:33.083968Z","iopub.execute_input":"2023-02-08T05:18:33.086001Z","iopub.status.idle":"2023-02-08T05:18:36.088959Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### And now take k = 6 as our optimal cluster:","metadata":{}},{"cell_type":"code","source":"k6<-kmeans(customer_data[,3:5],6,iter.max=100,nstart=50,algorithm=\"Lloyd\")\nk6","metadata":{"execution":{"iopub.status.busy":"2023-02-08T05:18:36.092805Z","iopub.execute_input":"2023-02-08T05:18:36.094609Z","iopub.status.idle":"2023-02-08T05:18:36.119083Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### We see several expressions in the output of the k-means operation, from which we can conclude useful information:\n- cluster – This is a vector of several integers that denote the cluster which has an allocation of each point.\n- totss – This represents the total sum of squares.\n- centers – Matrix comprising of several cluster centers\n- withinss – This is a vector representing the intra-cluster sum of squares having one component per cluster.\n- tot.withinss – This denotes the total intra-cluster sum of squares.\n- betweenss – This is the sum of between-cluster squares.\n- size – The total number of points that each cluster holds.","metadata":{}},{"cell_type":"markdown","source":"# Visualizing the Clustering Results","metadata":{}},{"cell_type":"code","source":"# Visualizing the Clustering Results using the First Two Principle Components\npcclust=prcomp(customer_data[,3:5],scale=FALSE) #principal component analysis\nsummary(pcclust)\n\npcclust$rotation[,1:2]","metadata":{"execution":{"iopub.status.busy":"2023-02-08T05:18:36.121477Z","iopub.execute_input":"2023-02-08T05:18:36.122908Z","iopub.status.idle":"2023-02-08T05:18:36.151743Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Visualize the clusters\nset.seed(1)\nggplot(customer_data, aes(x =Annual.Income..k.., y = Spending.Score..1.100.)) + \n  geom_point(stat = \"identity\", aes(color = as.factor(k6$cluster))) +\n  scale_color_discrete(name=\" \",\n              breaks=c(\"1\", \"2\", \"3\", \"4\", \"5\",\"6\"),\n              labels=c(\"Cluster 1\", \"Cluster 2\", \"Cluster 3\", \"Cluster 4\", \"Cluster 5\",\"Cluster 6\")) +\n  ggtitle(\"Segments of Mall Customers\", subtitle = \"Using K-means Clustering\")","metadata":{"execution":{"iopub.status.busy":"2023-02-08T05:18:36.154204Z","iopub.execute_input":"2023-02-08T05:18:36.155672Z","iopub.status.idle":"2023-02-08T05:18:36.598277Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### From the above visualization, we observe that there is a distribution of 6 clusters as follows:\n\n    - Clusters 6 and 4 – These clusters represent the customer_data with the medium income salary as well as the medium annual spend of salary.\n\n    - Cluster 1 – This cluster represents the customer_data having a high annual income as well as a high annual spend.\n\n    - Cluster 3 – This cluster denotes the customer_data with low annual income as well as low yearly spending of income.\n\n    - Cluster 2 – This cluster denotes a high annual income and low yearly spending.\n\n    - Cluster 5 – This cluster represents a low annual income but a high yearly expenditure.","metadata":{}},{"cell_type":"markdown","source":"# Segments of Mall Customers","metadata":{}},{"cell_type":"code","source":"ggplot(customer_data, aes(x =Spending.Score..1.100., y =Age)) + \n  geom_point(stat = \"identity\", aes(color = as.factor(k6$cluster))) +\n  scale_color_discrete(name=\" \",\n                      breaks=c(\"1\", \"2\", \"3\", \"4\", \"5\",\"6\"),\n                      labels=c(\"Cluster 1\", \"Cluster 2\", \"Cluster 3\", \"Cluster 4\", \"Cluster 5\",\"Cluster 6\")) +\n  ggtitle(\"Segments of Mall Customers\", subtitle = \"Using K-means Clustering\")","metadata":{"execution":{"iopub.status.busy":"2023-02-08T05:23:15.838177Z","iopub.execute_input":"2023-02-08T05:23:15.843886Z","iopub.status.idle":"2023-02-08T05:23:16.308996Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"kCols=function(vec){cols=rainbow (length (unique (vec)))\nreturn (cols[as.numeric(as.factor(vec))])}\ndigCluster<-k6$cluster; dignm<-as.character(digCluster); # K-means clusters\nplot(pcclust$x[,1:2], col =kCols(digCluster),pch =19,xlab =\"K-means\",ylab=\"classes\")\nlegend(\"bottomleft\",unique(dignm),fill=unique(kCols(digCluster)))","metadata":{"execution":{"iopub.status.busy":"2023-02-08T05:56:05.985507Z","iopub.execute_input":"2023-02-08T05:56:05.988093Z","iopub.status.idle":"2023-02-08T05:56:06.132005Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### From the above visualization, we observe that there is a distribution of 6 clusters as follows:\n\n- Cluster 4 and 1 – These two clusters consist of customers with medium PCA1 and medium PCA2 score.\n\n- Cluster 6 – This cluster represents customers having a high PCA2 and a low PCA1.\n\n- Cluster 5 – In this cluster, there are customers with a medium PCA1 and a low PCA2 score.\n\n- Cluster 3 – This cluster comprises of customers with a high PCA1 income and a high PCA2.\n\n- Cluster 2 – This comprises of customers with a high PCA2 and a medium annual spend of income.","metadata":{}},{"cell_type":"markdown","source":"# Summary\n- With the help of clustering, we can understand the variables much better, prompting us to take careful decisions. With the identification of customers, companies can release products and services that target customers based on several parameters like income, age, spending patterns, etc. Furthermore, more complex patterns like product reviews are taken into consideration for better segmentation.","metadata":{}}]}